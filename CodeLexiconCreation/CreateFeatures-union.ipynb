{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authors: Tlachac, et al\n",
    "#Paper: \"Automated Construction of Lexicons to Improve Depression Screening with Text Messages\"\n",
    "\n",
    "from empath import Empath\n",
    "from empath import helpers as util\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = ['e1526', 'e2806', 'e3702', 'e4894', 'e5006', 'e5199', 'e9137', 'e9237', 'e9513', 'e9766', 'm1087', 'm12', 'm1206', 'm1226', 'm1281', 'm1399', 'm1487', 'm1506', 'm1762', 'm1836', 'm1957', 'm1960', 'm2109', 'm2132', 'm2173', 'm2185', 'm2216', 'm2331', 'm2374', 'm2402', 'm2561', 'm2564', 'm2892', 'm2993', 'm3234', 'm3552', 'm3616', 'm3810', 'm4088', 'm4238', 'm4368', 'm4435', 'm4506', 'm4580', 'm473', 'm4814', 'm4937', 'm4947', 'm5223', 'm5464', 'm5487', 'm5666', 'm5882', 'm5926', 'm5989', 'm6499', 'm676', 'm6951', 'm7101', 'm7123', 'm7124', 'm7237', 'm7340', 'm746', 'm7490', 'm7856', 'm7861', 'm7974', 'm8075', 'm8080', 'm8131', 'm8338', 'm8555', 'm8595', 'm8676', 'm8833', 'm8921', 'm8979', 'm9014', 'm9019', 'm9074', 'm9301', 'm9505', 'm9525', 'm9751', 'm9886', 'm9928', 'm9984']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data frames\n",
    "dataFrame = pd.read_csv(\"sent14days.csv\")\n",
    "\n",
    "# obtain ID, texts, scores, and address 2 from dataframe\n",
    "dataFrame = dataFrame[['id', 'body2', 'scores', 'address2']]\n",
    "\n",
    "dataFrame = dataFrame[dataFrame['id'].isin(participants)].reset_index()\n",
    "dataFrame['body2'] = dataFrame['body2'].astype(str)\n",
    "\n",
    "# Group data frame by participant ID \n",
    "def sortList(inList):\n",
    "    newList = \" \".join(inList)\n",
    "    return newList\n",
    "\n",
    "def scoreList(inList): \n",
    "    #print(list(inList))\n",
    "    if len(inList) == 0: \n",
    "        return None\n",
    "    else: \n",
    "        return list(inList)[0]\n",
    "\n",
    "#function to clean out punctuation and make text lowercase\n",
    "def cleanText(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation,\"\")\n",
    "    return text.lower()\n",
    "\n",
    "newCol = dataFrame[['id', 'body2']].groupby(by = \"id\").agg(len)\n",
    "\n",
    "dataFrame = dataFrame.sort_values(by=\"id\")\n",
    "dataFrame = dataFrame.groupby(by = \"id\").agg({\"body2\":sortList, \"scores\": scoreList})\n",
    "dataFrame['NumTexts'] = newCol\n",
    "\n",
    "dataFrame.reset_index(inplace=True)\n",
    "\n",
    "#actively remove punctuation and make lower case for text messages\n",
    "dataFrame[\"body2\"] = dataFrame[\"body2\"].apply(cleanText)\n",
    "\n",
    "print(dataFrame.shape)\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of all empath categories\n",
    "\n",
    "lexicon = Empath()\n",
    "emp = lexicon.analyze(\"Testing\", normalize=True)\n",
    "wordlist = []\n",
    "for word, value in emp.items():\n",
    "    wordlist.append(word)\n",
    "print(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = [\"1c\", \"1f\", \"1r\", \"3c\", \"3f\", \"3r\", \"5c\", \"5f\", \"5r\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for par in paras:\n",
    "    print(par)\n",
    "    lexF = pd.read_csv(\"wordsFiction\" + par + \".csv\")\n",
    "    lexR = pd.read_csv(\"wordsReddit\" + par + \".csv\")\n",
    "    lexN = pd.read_csv(\"wordsNews\" + par + \".csv\")\n",
    "\n",
    "    #walk through each category\n",
    "    for j in range(0, len(lexF.words)):\n",
    "    \n",
    "        savelistF = lexF.words[j][1:-1].split(\", \")\n",
    "        savelistF = [s[1:-1] for s in savelistF]\n",
    "        \n",
    "        savelistR = lexR.words[j][1:-1].split(\", \")\n",
    "        savelistR = [s[1:-1] for s in savelistR]\n",
    "        \n",
    "        savelistN = lexN.words[j][1:-1].split(\", \")\n",
    "        savelistN = [s[1:-1] for s in savelistN]\n",
    "        \n",
    "        savelist = list(set(savelistF + savelistR + savelistN))\n",
    "\n",
    "        print(savelist)\n",
    "        \n",
    "        frequencies = []\n",
    "        #walk through each participant\n",
    "        for i in range(0,dataFrame.shape[0]):\n",
    "            body = dataFrame.body2[i]\n",
    "            #n words in body\n",
    "            bodylen = len(body.split(\" \"))\n",
    "\n",
    "            c = 0\n",
    "            for w in savelist: \n",
    "                #skip over empty strings that used to be punctuation\n",
    "                if w != \"\":\n",
    "                    c = c + body.count(\" \" + w + \" \")\n",
    "\n",
    "            frequencies.append(c/bodylen)\n",
    "        dataFrame[wordlist[j]] = frequencies\n",
    "    dataFrame.to_csv(\"featuresCombined\" + par + \".csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
