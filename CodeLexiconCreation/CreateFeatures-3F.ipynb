{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authors: Tlachac, et al\n",
    "#Paper: \"Automated Construction of Lexicons to Improve Depression Screening with Text Messages\"\n",
    "\n",
    "from empath import Empath\n",
    "from empath import helpers as util\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries and empath class \n",
    "from collections import defaultdict\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "import json\n",
    "class Empath:\n",
    "    def __init__(self, backend_url=\"http://54.148.189.209:8000\"):\n",
    "        self.cats = defaultdict(list)\n",
    "        self.staging = {}\n",
    "        self.backend_url = backend_url\n",
    "        self.base_dir = os.path.dirname(util.__file__)\n",
    "        #print(self.base_dir)\n",
    "        self.inv_cache = {}\n",
    "        #loads the default empath categories \n",
    "        self.load(self.base_dir+\"/data/categories.tsv\")\n",
    "        # loads the user-written categories \n",
    "        for f in os.listdir(self.base_dir+\"/data/user/\"):\n",
    "            if len(f.split(\".\")) > 1 and f.split(\".\")[1] == \"empath\":\n",
    "                self.load(self.base_dir+\"/data/user/\"+f)\n",
    "    def load(self,file):\n",
    "        with open(file,\"r\") as f:\n",
    "            for line in f:\n",
    "                cols = line.strip().split(\"\\t\")\n",
    "                name = cols[0]\n",
    "                terms = cols[1:]\n",
    "                for t in set(terms):\n",
    "                    self.cats[name].append(t)\n",
    "                    #self.invcats[t].append(name)\n",
    "    def analyze_term_window(self,doc,targets,categories=None,window_size=10,normalize=False):\n",
    "        tokenizer = util.window_tokenizer(window_size,targets)\n",
    "        return self.analyze(doc,categories,tokenizer,normalize)\n",
    "    def analyze(self,doc,categories=None,tokenizer=\"default\",normalize=False):\n",
    "        if isinstance(doc,list):\n",
    "            doc = \"\\n\".join(doc)\n",
    "        if tokenizer == \"default\":\n",
    "            tokenizer = util.default_tokenizer\n",
    "        elif tokenizer == \"bigrams\":\n",
    "            tokenizer = util.bigram_tokenizer\n",
    "        if not hasattr(tokenizer,\"__call__\"):\n",
    "            raise Exception(\"invalid tokenizer\")\n",
    "        if not categories:\n",
    "            categories = self.cats.keys()\n",
    "        invcats = defaultdict(list)\n",
    "        key = tuple(sorted(categories))\n",
    "        if key in self.inv_cache:\n",
    "            invcats = self.inv_cache[key]\n",
    "        else:\n",
    "            for k in categories:\n",
    "                for t in self.cats[k]: invcats[t].append(k)\n",
    "            self.inv_cache[key] = invcats\n",
    "        count = {}\n",
    "        tokens = 0.0\n",
    "        for cat in categories: count[cat] = 0.0\n",
    "        for tk in tokenizer(doc):\n",
    "            tokens += 1.0\n",
    "            for cat in invcats[tk]:\n",
    "                count[cat]+=1.0\n",
    "        if normalize:\n",
    "            for cat in count.keys():\n",
    "                if tokens == 0:\n",
    "                    return None\n",
    "                else:\n",
    "                    count[cat] = count[cat] / tokens\n",
    "        return count\n",
    "    def create_category(self,name,seeds,model=\"fiction\",size=100,write=True):\n",
    "        resp = requests.post(self.backend_url + \"/create_category\", json={\"terms\":seeds,\"size\":size,\"model\":model})\n",
    "        return(resp.text)\n",
    "        print(resp.text)\n",
    "        results = json.loads(resp.text)\n",
    "        self.cats[name] = list(set(results))\n",
    "        if write:\n",
    "            with open(self.base_dir+\"/data/user/\"+name+\".empath\",\"w\") as f:\n",
    "                f.write(\"\\t\".join([name]+results))\n",
    "    def delete_category(self,name):\n",
    "        if name in self.cats: del self.cats[name]\n",
    "        filename = self.base_dir+\"/data/user/\"+name+\".empath\"\n",
    "        if os.path.isfile(filename):\n",
    "            os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates \"wordsToUse\" which are seed words for each category \n",
    "\n",
    "# Open the category file \n",
    "catFile = open(\"categories.tsv\", \"r\", encoding=\"utf-8\")\n",
    "\n",
    "# variable to store the seed words that I created \n",
    "wordList = []\n",
    "wordsToUse = []\n",
    "nWords = []\n",
    "\n",
    "j = 0 \n",
    "# loops through each line in the file \n",
    "for line in catFile: \n",
    "    categoryList = line.strip(\"\\n\").split(\"\\t\")\n",
    "    \n",
    "    nWords.append(len(categoryList))\n",
    "    categoryName = categoryList[0]\n",
    "    wordList.append(categoryName)\n",
    "    wordsToUse.append(categoryList[1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lexicon = Empath()\n",
    "savelist = []\n",
    "# Create own categories (using reddit model)\n",
    "myReddit5C = zip(wordList, wordsToUse, nWords)\n",
    "for wlist, wordsused, nwords in list(myReddit5C):\n",
    "    print(wlist)\n",
    "    #print(wordsused)\n",
    "    #print(nwords)\n",
    "    #print(\"_________________________\")\n",
    "    save = lexicon.create_category(wlist, wordsused, model = \"fiction\", size = nwords)\n",
    "    tempsave = []\n",
    "    nCounter = 5\n",
    "    for indword in save[1:-1].split(\",\"):\n",
    "        temp = indword.replace('\"', \"\").strip()\n",
    "        #remove punctuation\n",
    "        tempsave.append(re.sub(r'[^\\w\\s]', '', str(temp).lower()))\n",
    "    for indseed in wordsused:\n",
    "        if indseed in tempsave:\n",
    "            nCounter = nCounter - 1\n",
    "            #print(nCounter)\n",
    "        else:\n",
    "            tempsave.insert(0, indseed)    \n",
    "    tempsave.insert(0, wlist)\n",
    "    savelist.append(list(set(tempsave)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data frame \n",
    "dataFrame = pd.read_csv(\"sent14days.csv\")\n",
    "#print(dataFrame)\n",
    "\n",
    "# obtain ID, texts, scores, and address 2 from dataframe\n",
    "dataFrame = dataFrame[['id', 'body2', 'scores', 'address2']]\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data frame by participant ID \n",
    "def sortList(inList):\n",
    "    newList = \" \".join(inList)\n",
    "    return newList\n",
    "\n",
    "def scoreList(inList): \n",
    "    #print(list(inList))\n",
    "    if len(inList) == 0: \n",
    "        return None\n",
    "    else: \n",
    "        return list(inList)[0]\n",
    "\n",
    "#function to clean out punctuation and make text lowercase\n",
    "def cleanText(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation,\"\")\n",
    "    return text.lower()\n",
    "\n",
    "newCol = dataFrame[['id', 'body2']].groupby(by = \"id\").agg(len)\n",
    "\n",
    "dataFrame = dataFrame.sort_values(by=\"id\")\n",
    "dataFrame = dataFrame.groupby(by = \"id\").agg({\"body2\":sortList, \"scores\": scoreList})\n",
    "dataFrame['NumTexts'] = newCol\n",
    "\n",
    "#only 2+ messages\n",
    "dataFrame = dataFrame[dataFrame['NumTexts'] >= 2]\n",
    "\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame.reset_index(inplace=True)\n",
    "\n",
    "#actively remove punctuation and make lower case for text messages\n",
    "dataFrame[\"body2\"] = dataFrame[\"body2\"].apply(cleanText)\n",
    "\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataFrame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#walk through each word\n",
    "for j in range(0, len(savelist)):\n",
    "\n",
    "    frequencies = []     \n",
    "        \n",
    "    #walk through each participant\n",
    "    for i in range(0,dataFrame.shape[0]):\n",
    "        body = dataFrame.body2[i]\n",
    "        #n words in body\n",
    "        bodylen = len(body.split(\" \"))\n",
    "    \n",
    "        c = 0\n",
    "        for w in savelist[j]: \n",
    "            #skip over empty strings that used to be punctuation\n",
    "            if w != \"\":\n",
    "                c = c + body.count(\" \" + w + \" \")\n",
    "                \n",
    "        frequencies.append(c/bodylen)\n",
    "    dataFrame[wordList[j]] = frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame.to_csv(\"featuresFiction3f.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF = pd.DataFrame()\n",
    "newDF[\"categories\"] = wordList\n",
    "newDF[\"seedwords\"] = wordsToUse\n",
    "newDF[\"words\"] = savelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF.to_csv(\"wordsFiction3f.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
